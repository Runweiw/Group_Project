---
title: "506_project_FlexMix"
author: "Lingke Ying"
date: "11/27/2018"
output:
  html_document: default
  pdf_document: default
---

### 0. General information

In this tutorial, we will use the package `FlexMix` in R to fit finite mixtures of regression models. `FlexMix` implements a general framework for fitting discrete mixtures of regression models using the EM algorithm in the R statistical computing environment. We use it to complete a model-based clustering.
```{r general, message = FALSE}
library(stringr)
library(flexmix)
library(data.table)
```

### 1. Download Wine data and save to the current working directory
We first download Wine dataset from the UCI Machine Learning Repository and save the dataset to the current working directory in R. We use `read.delim` to read the `wine.txt` file, and assign names to each column.

```{r dataset}
wine = read.delim("./wine.txt", sep = ",", header = FALSE)
a = c("Class", "Alcohol", "Malic_acid", "Ash", "Alcalinity_of_ash", "Magnesium", 
      "Total_phenols", "Flavanoids", "Nonflavanoid_phenols", "Proanthocyanins", 
      "Color_intensity", "Hue", "OD280/OD315_of_diluted_wines", "Proline")
names(wine) = a
```

### 2. Pattern of the selected variables

Then we explore the patterns of variables. The using variables in this tutorial are `Color_intensity`, `Total_phenols`, and `Class`, where `Color_intensity` is the response and `Total_phenols` is the predictor in this model. To explore the patterns, we plot below the density plot of `Color_intensity`, the density plot of `Total_phenols`, and the histogram of the ground truth class `Class`.

```{r pattern}
# - density plot of response "Color_intensity"
d_color = density(wine$Color_intensity)
plot(d_color, main="Density of Color_intensity")
# - density plot of predictor "Total_phenols"
d_phenol = density(wine$Total_phenols)
plot(d_phenol, main="Density of Total_phenols")
# - histogram of the ground truth class "Class"
h_class = hist(wine$Class)
```

We see from the graph that the distribution of `Color_intensity` is right-skewed, and the distribution of `Total_phenols` has two evident peaks. The histogram of `Class` shows obervations in each Class. We now have 59, 71, and 48 observations in Class 1, 2, and 3.


### 3. Set initial values for EM iterations

We may want to set the initial values for EM iterations. In our Stata model, the initial values are setup by using `emopts(iterate(0)) noestimate` when calling fmm. 
Below are the initial values of `lembda`, `beta`, and `sigma`.

```{r init, message = FALSE}
lambda = c(0.3370787, 0.3314607, 0.3314607)
beta = matrix(c(2.6478173, 2.3281364, 3.2570108, -3.3001476, 2.9323563, -4.4052074), nrow = 2)
sigma = sqrt(c(5.3676108, .41571224, .92670911))
```

```{r, message = FALSE}
lambda
beta
sigma
```

However, the limit of the package `FlexMix` is that only the default initial value can be applied. Not being able to change the initial values of `lembda`, `beta` and `sigma`, we decide to keep them as default (for example, lambda = 1/3, 1/3, 1/3). Moreover, the default model we use in this package is `FLXMRglm()`, which is the main driver for `FlexMix` interfacing the glm family of models.

### 4. Results of model fitting
Then we fitted the model on the data, with regression of `Color_intensity` on `Total_phenols`. Three kinds of Class in this model represent three underlying subpopulations. Here below are the model fitting results and the parameters.

```{r echo = FALSE, message = FALSE}
set.seed(155)  # Set seed for repeatable experiment
```

```{r results}
m1 = flexmix(Color_intensity ~ Total_phenols, data = wine, k = 3)
print(summary(m1))
knitr::kable(parameters(m1), align = 'c', caption = 'Table 1: Parameters of the fitted model.')
```

### 5. Clustering

Here we get the posterior probabilities of cluster. We then generate a new variable `Cluster` to store the information of assigned cluster numbers. It's worth noting that `m1` is an `S4 object`, so we should use `@` to retrieve the attributes.

```{r cluster}
# m1 - S4 object, use @ to retrieve the attributes
Cluster = m1@cluster
post = cbind('Class' = wine$Class,
             'pr1' = m1@posterior$scaled[,1],
             'pr2' = m1@posterior$scaled[,2],
             'pr3' = m1@posterior$scaled[,3])
knitr::kable(head(post), align = 'c', caption = 'Table 2: First 6 lines of posterior probabilities.')

cluster = table(Cluster)
knitr::kable(cluster, align = 'c', caption = 'Table 3: Number of cases in each cluster.')
```

### 6. Classification
We study the aggregation table of medians of posterior probabilities by Class, and found that a good match is Cluster 1, 2, and 3 with Class 1, 2, and 3, correspondingly.

```{r message = FALSE}
# median of posterior probabilities, by "Class"
dt = data.table(post)
med = dt[, .(med1 = median(pr1), med2 = median(pr2), med3 = median(pr3)), Class]
knitr::kable(med, align = 'c', caption = 'Table 4: Median of posterior probabilities, by "Class".')
```

### 7. Accuracy of the classification
We calculate the accuracy of the classification and create a confusion matrix. The accuracy of our fitted model is 0.719. From the confusion matrix, we can expore how well the the model fits. 
```{r error}
# Accuracy
Class = wine$Class
accuracy = sum(Class == Cluster)/178
print(accuracy)

# Confusion matrix
C_mat = table(Cluster, Class)
knitr::kable(C_mat, align = 'c', caption = 'Table 5: Confusion matrix of classification results.') 
```

